---
title: "A Feature Selection and Extraction"
output: 
  html_document:
    highlight: tango
    toc: true
    toc_float:
      collapsed: false
---


<div class="row" style="padding-top: 100px;">
</div>




##########################################################################

# The concept

## An example

```{r echo=FALSE}
require(plotrix)
```

Imagine that we are dealing with Gaia RVS spectra: 
```{r echo=FALSE}
n <- 10000
```

These spectra are characterized by a wavelength coverage of 845â€“872 nm with R=11500. 

![](images/RVSexample.jpg)

As you can see, the range is small but contains a number of spectral lines that are important diagnostics of the thermodynamic state of the stellar atmosphere.

Let us leave the physical analysis of the spectral lines for now and concentrate in the problem of handling feature spaces of the size of an RVS spectrum. It typically contains circa `r n` fluxes, many of which contain no information about the stellar physical parameters. This represents a `r n`-dimensional space. 

Now, imagine a set of example RVS spectra from stars of many different temperatures (surface gravities, metallicities...). Our aim is to construct a model that predicts temperatures from an observed spectrum. Let us do it using one of the most simple models at hand: the k-nearest neighbours (kNN). kNN will receive an observed spectrum and look for the k closest spectra in the set of examples. What do you think would be a reasonable value for k? Let us start with k=1: we will attach to the observed spectrum the temperature of the example that is closest in some to-be-determined sense.

Now, for the sake of clarity, let us rescale each observed flux between 0 and 1. This is not necessary, but helps visualize the problem. We, of course, use the same rescaling for the observed spectrum and for the training set.

After rescaling, our data set occupies a hypercube of `r n`dimensions with unit side. 

Now, let us ask ourselves how many examples do we need to ensure that the closest example (in the to-be-determined sense) is not further than one particular example: one example that differs from our observed spectrum by a 10% in each flux. 

If our spectra had only 2 measured fluxes, this furthest allowed example would be at a distance equal to $\sqrt{0.1^2+0.1^2}$=`r sqrt(0.1^2+0.1^2)`.

```{r}
plot(cbind(0,0),xlim=c(0,1),ylim=c(0,1),xlab="feature #1",ylab="feature #2",col="white")
points(cbind(0.6,0.3),pch=16,col="blue")
r <- sqrt(0.1^2+0.1^2)
draw.circle(0.6,0.3,r)
```

How many examples do we need to ensure that there is always an example within a radius of `r sqrt(0.1^2+0.1^2)`? Well, the total area of the square is 1, and that of the circle is $\pi*r^2$=`r pi*r^2` so we need $\frac{1}{\pi*r^2}$=`r 1/(pi*r^2)`.

That is roughly 16 examples distributed homogeneously in the 2D square.

Let us now look at the true RVS space. The hypercube volume still is 1, but the volume of the hypersphere in $n$ dimensions is now $\frac{2\cdot r^d*\pi^{n/2}}{\Gamma(n/2)}$. Our RVS spectra have `r n` fluxes,  so if we still enforce that the farthest example is at most at a 10% in each dimension, the examples have to be closer than $$d=\sqrt{\sum_{i=1}^{10000} 0.1^2}.$$ That is, there has to be an example at a distance d<`r sqrt(10000*0.1^2)`.

How many examples do we need to ensure that there is at least one within a circle of radius `r sqrt(10000*0.1^2)`? 

A hypersphere in `r n` dimensions of radius `r sqrt(10000*0.1^2)` has a volume equal to 
```{r} 
r <- sqrt(10000*0.1^2)
logVsphere <- log(2)+n*log(r)+(n/2)*log(pi)-lgamma(n/2)
print(logVsphere)
```

So (in the hypothesis of training examples distributed homogeneously in the hypercube), we would need 

$$\frac{1}{\exp(logVsphere)} $$ examples or, in natural logarithmic units (otherwise I get an infinity in R)

we would need 10^`r as.integer(log(1)-logVsphere)` examples. This is just unfeasible.

So if we go kNN, it is unthinkable that we can gather a large enough set of training examples such that the closest example is at a distance shorter than that implied by a 10% difference in each feature. A10%! That is a lot.

There are other ways to visualize this curse-of-dimensionality. But in practice they all imply an exponentially increasing training set size with increasing space dimensionality. Or conversely, it implies that for a fixed training set size, the predictive power degrades with the dimensionality of the space (the so-called Hughes phenomenon). 



#######################################################################

# The concept

Once we have seen that high-dimensional spaces can be a curse, we need criteria to reduce the dimensionality of the input space without losing important information. And here is the difficult bit: to go from the loose semantic content of "important" to something that can be made reproducible and analytic. 

The two concepts that are key to this notebook are relevance and redundancy

# The example

Let us play with a toy problem in which our feature space is composed of 5 boolean variables ($X_1, X_2, X_3, X_4$, and, $X_5$) and the class is defined as $$C=X_1 \oplus X_2 \oplus X_3$$. 

Then, $X_4$ is defined as $X_2 \oplus X_3$. Finally, $X_5$ is a random variable that takes boolean values independent of any of the other variables $X_1-X_4$.  Therefore, the class is independent of $X_5$. 

Here you have the only 16 possible examples:

```{r echo = FALSE, results = 'asis'}
x1 <- c(0,1)
x2 <- c(0,1)
x3 <- c(0,1)
x5 <- c(0,1)
cases <- expand.grid(x1,x2,x3,x5)
x4 <- as.integer(xor(cases[,2],cases[,3]))
cases <- cbind(cases[,1],cases[,2],cases[,3],x4,cases[,4])
class <- as.integer(xor(cases[,1],cases[,4]))
cases <- cbind(cases,class)
colnames(cases) <- c("x1","x2","x3","x4","x5","class")
library(knitr)
kable(cases,col.names = c("x1","x2","x3","x4","x5","class"))
```


This is a very simple, artificial example defined to illustrate some of the key concepts of dimensionality reduction. From God's perspective, the solution is clear: keep $X_1$ and $X_4$ and make sure that your model is capable of capturing highly non-linear and disconnected boundaries such as those implied by this problem.


```{r}

plot(cases[,1],cases[,4],xlab=expression(X[1]), ylab=expression(X[4]),pch=16,cex=3,col=class+1,xlim=c(-.5,1.5), ylim=c(-.5,1.5))

```


The set $\{X_1, X_4\}$ is at the same time sufficient and minimal. There are other alternatives: $\{X_1, X_2, X_3\}$ is sufficient (but not minimal). There is redundancy between $X_2, X_3 and X_4$, and $X_5$ is completely irrelevant for the determination of the class. Unfortunately, we hardly ever are God sitting on a cloud, and we never know an analytical expression for the class.

Now, for the definitions. This is the particular set I (and a vast majority of the community) agree with. There are [alternatives](https://www.researchgate.net/profile/Ron_Kohavi/publication/223713209_Wrappers_for_Feature_Subset_Selection/links/02e7e51bcc03dd7eef000000.pdf?origin=publication_detail), of course.


* $Y$ represents the target variable, while $y$ represents a particular possible value.
* $X$ represents features (random variables), while $s$ represent particular values of the variable
* Let $S_i$ be the set of all features except $X_i$
* Let $S'_i$ be a subset of $S_i$
* A feature $X_i$ is strongly relevant if and only if there exists $x_i$, $y$, and $s_i$ for which $$p(X_i = x_i, S_i = s_i) > 0$$ such that $$p(Y=y|X_i=x_i,S_i=s_i) \neq p(Y=y|S_i=s_i)$$
* A feature $X_i$ is weakly relevant if and only if it is not strongly relevant and there exists $x_i$, $y$, and $s'_i$ for which $$p(X_i = x_i, S'_i = s'_i) > 0$$ such that $$p(Y=y|X_i=x_i,S'_i=s'_i) \neq p(Y=y|S'_i=s'_i)$$
* A feature $X_i$ is irrelevant if it is not relevant (in any sense).

## Redundancy 

In our example, there is redundancy between the two sets $\{X_4\}$ and $\{X_2,X_3\}$. We can build two decision trees that  

``` {r }
set.seed(0)
require("bnlearn")
data1 <- rbind(cases,cases,cases,cases)
data2 <- as.integer(runif(64)*10)
data2 <- cases[data2,]

data1 <- as.data.frame(apply(data1,2,as.factor))
data2 <- as.data.frame(apply(data2,2,as.factor))

sum(data1[,5]==0 & data1[,6] == 0)
sum(data1[,5]==1 & data1[,6] == 0)
sum(data1[,5]==0 & data1[,6] == 1)
sum(data1[,5]==1 & data1[,6] == 1)

sum(data2[,5]==0 & data2[,6] == 0)
sum(data2[,5]==1 & data2[,6] == 0)
sum(data2[,5]==0 & data2[,6] == 1)
sum(data2[,5]==1 & data2[,6] == 1)

res = tabu(data1,score="bic")
plot(res)
fitted <- bn.fit(res,data=data1)
summary(fitted)
cpquery(fitted,(class==1),(x1=="1" & x4=="0"))

res = tabu(data2,score="bic")
plot(res)
fitted <- bn.fit(res,data=data1)
summary(fitted)
cpquery(fitted,(class==1),(x1=="1" & x4=="0"))

```

Hence, one aims at Minimum-redundancy-maximum-relevance (mRMR) feature selection

This can be implemented in practice in several ways, depending on the way we measure relevance and redundancy. More on this soon. Stay tuned!

## Optimality and relevance.


#######################################################################


## The difference between Feature Extraction and Feature Selection


#######################################################################




## The RVS datasets

### The first dataset

In the following, we will apply a series of feature selection or feature extraction techniques to a dataset taken from the Gaia simulations. This dataset consists of 1000 spectra synthesized from the MARCS [http://marcs.astro.uu.se/] library of stellar atmospheric models. 

This is not the full dataset used to train the Machine Learning algorithms that will run as part of the Gaia Data Processing and Analysis. Only a tiny subset selected to be at the same time illuminating for you, and manageable within the school lecture durations.

It was selected by forcing the stellar metallicities and alpha abundances to be solar, and then sampling randomly in magnitude. Furthermore, the radial velocities and rotational velocities are negligible.

Finally, all spectra have been normalized to unit area. That is, the area under each spectrum is 1 (in the somewhat awkward units of pixel position). We have done this because the actual value of the observed fluxes carry no information on any of the physical parameters (this is not exactly so, but let's assume it for the sake of simplicity). We could have exactly the same star at two wildly different distances and therefore, very different observed fluxes. 

OK. Let us load the spectra and have a first grasp of how they look like.

```{r }

StPa <- read.table("stellarParameters.dat",colClasses="numeric",sep=",")
names(StPa) <- c("teff","logg","met")
Spec <- read.table("spectra.dat",sep=",")

library(scales)
library(fields)

pal <- colorRampPalette(c("yellow","red"),space="rgb")
SetupPalette<-function(c)
{
  nl <- 50
  palette<-pal(nl)
  #palette <- tim.colors(nl)
  #palette <- heat.colors(nl)
  col <- c-min(c,na.rm=TRUE)
  col <- col/max(col,na.rm=TRUE)
  colour <- palette[as.integer(((nl-1)*col)+1)]
  return(colour)
}

n <- dim(Spec)[1]
col <- SetupPalette(StPa[,1])

wv <- as.integer(10*read.table("wavelengths.dat")[1,90:1150])

par(mar=c(6,6,6,6))
plot(as.numeric(Spec[1,]),ty="l",col=alpha(col[1],0.2),ylim=c(0.0005,0.001), xlab="Pixel Number",ylab="Normalized Flux")

axis(3,seq(0,1000,by=200),labels=wv[1+seq(0,1000,by=200)])

for (i in 2:n)
{
  lines(as.numeric(Spec[i,]),col=alpha(col[i],0.2))
}

image.plot(legend.only=TRUE, zlim= range(StPa[,1],na.rm=T), horizontal=FALSE,
           legend.width=2, reset.graphics=TRUE, axis.args=list(cex.axis=1,cex.lab=1),
           legend.mar=5,col=pal(50))
  

```

So we basically see two things: 

* There are lines that are only present in certain wavelength ranges. If you want a reasonable introduction to the behaviour of lines in stellar spectra, I certainly recommend you the books by D. Gray. For an identification of the lines present in the RVS range, you can have a look at the recent  (article by Recio-Blanco et al)[http://www.aanda.org/articles/aa/pdf/2016/01/aa25030-14.pdf]. 
* Even for the same temperature, there are very significant variations across spectra


Let us look at the other free variable: surface gravity.

``` {r}

mask <- StPa[,1] == 3.9

col <- SetupPalette(StPa[,2])

par(mar=c(6,6,6,6))
plot(as.numeric(Spec[mask,][1,]),ty="l",col=alpha(col[mask][1],0.2),ylim=c(0.0005,0.001), xlab="Pixel Number",ylab="Normalized Flux")

axis(3,seq(0,1000,by=200),labels=wv[1+seq(0,1000,by=200)])

for (i in 2:sum(mask))
{
  lines(as.numeric(Spec[mask,][i,]),col=alpha(col[mask][i],0.2))
}

image.plot(legend.only=TRUE, zlim= range(StPa[mask,2],na.rm=T), horizontal=FALSE,
           legend.width=2, reset.graphics=TRUE, axis.args=list(cex.axis=1,cex.lab=1),
           legend.mar=5,col=pal(50))

save.image()

```

Let us keep this very simplified dataset (40 spectra corresponding to stars of $log(T_{\rm eff})=3.9$ and only 6 different values of the surface gravity)

```{r}

small.spec <- Spec[mask,]
small.param <- StPa[mask,]
  
```


#######################################################################



```{r }

load(".RData")
library(FSelector)
library(scales)

teff <- StPa[,1]
df <- data.frame(cbind(Spec,teff))

# For discrete variables (try with the 2D/3D XOR problem)
#chi.squared(class~.,data1)

# 
score <- linear.correlation(teff~.,df)
col <- SetupPalette(score[,1])
par(mar=c(6,6,6,6))
plot(as.numeric(Spec[1,]),pch=".",col=alpha(col,0.2),ylim=c(0.0005,0.001), xlab="Pixel Number",ylab="Normalized Flux")



```

#######################################################################



```{r }

library(rpart)
subset.wrapperscore <- function(subset){

  #k-fold cross validation
  k <- 5
  splits <- runif(nrow(df))

  results = sapply(1:k, function(i) {
    test.idcs <- (splits >= (i - 1) / k) & (splits < i / k)
    train.idcs <- !test.idcs
    test <- df[test.idcs, , drop=FALSE]
    train <- df[train.idcs, , drop=FALSE]
#    model <- rpart(as.simple.formula(subset, "teff"), train)
   model <- lm(as.simple.formula(subset, "teff"), train)
#   model <- naive.bayes(train,"teff",subset)
    error.rate = sqrt(sum((test$teff-predict(model, test))^2) / nrow(test))
    return(-log10(error.rate))
  })
  
  #print(subset)
  #print(mean(results))
  return(mean(results))
}


```

#######################################################################



```{r }

#accu <- random.forest.importance(teff~., df, importance.type = 1)
#node.impu <- random.forest.importance(teff~., df, importance.type = 2)


```





